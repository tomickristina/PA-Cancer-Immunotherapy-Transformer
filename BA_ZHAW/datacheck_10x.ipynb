{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f0f8cc-21fd-4683-bde8-3f4752b0791a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115872/1185607799.py:35: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
      "/tmp/ipykernel_115872/1185607799.py:35: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Paired Gene ---\n",
      "Anzahl der Zeilen im Trainingsdatensatz: 61498 (Binding=1: 33711, Binding=0: 27787, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Anzahl der Zeilen im Testdatensatz: 36806 (Binding=1: 7226, Binding=0: 29580, TPP1: 22545, TPP2: 13972, TPP3: 289)\n",
      "Anzahl der Zeilen im Validierungsdatensatz: 36934 (Binding=1: 7224, Binding=0: 29710, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Gesamtanzahl der Zeilen (Train + Test + Validation): 135238\n",
      "\n",
      "--- Paired Allele ---\n",
      "Anzahl der Zeilen im Trainingsdatensatz: 66590 (Binding=1: 36515, Binding=0: 30075, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Anzahl der Zeilen im Testdatensatz: 39891 (Binding=1: 7826, Binding=0: 32065, TPP1: 25231, TPP2: 14579, TPP3: 81)\n",
      "Anzahl der Zeilen im Validierungsdatensatz: 40021 (Binding=1: 7826, Binding=0: 32195, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Gesamtanzahl der Zeilen (Train + Test + Validation): 146502\n",
      "\n",
      "--- Beta Gene ---\n",
      "Anzahl der Zeilen im Trainingsdatensatz: 251750 (Binding=1: 125875, Binding=0: 125875, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Anzahl der Zeilen im Testdatensatz: 161844 (Binding=1: 26974, Binding=0: 134870, TPP1: 140580, TPP2: 20961, TPP3: 298)\n",
      "Anzahl der Zeilen im Validierungsdatensatz: 161838 (Binding=1: 26973, Binding=0: 134865, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Gesamtanzahl der Zeilen (Train + Test + Validation): 575432\n",
      "\n",
      "--- Beta Allele ---\n",
      "Anzahl der Zeilen im Trainingsdatensatz: 279290 (Binding=1: 139645, Binding=0: 139645, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Anzahl der Zeilen im Testdatensatz: 179550 (Binding=1: 29925, Binding=0: 149625, TPP1: 157873, TPP2: 21581, TPP3: 94)\n",
      "Anzahl der Zeilen im Validierungsdatensatz: 179532 (Binding=1: 29922, Binding=0: 149610, TPP1: 0, TPP2: 0, TPP3: 0)\n",
      "Gesamtanzahl der Zeilen (Train + Test + Validation): 638372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Beispielpfade für Train-, Test-, und Validierungsdatensätze für alle vier Kategorien\n",
    "base_path = 'data_10x/splitted_datasets'\n",
    "precision = 'allele' #allele or gene\n",
    "\n",
    "# Definierte Pfade für alle vier Kategorien\n",
    "datasets = {\n",
    "    \"paired_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/paired/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/paired/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/paired/validation.tsv\"\n",
    "    },\n",
    "    \"paired_allele\": {\n",
    "        \"train\": f\"{base_path}/allele/paired/train.tsv\",\n",
    "        \"test\": f\"{base_path}/allele/paired/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/allele/paired/validation.tsv\"\n",
    "    },\n",
    "    \"beta_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/beta/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/beta/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/beta/validation.tsv\"\n",
    "    },\n",
    "    \"beta_allele\": {\n",
    "        \"train\": f\"{base_path}/allele/beta/train.tsv\",\n",
    "        \"test\": f\"{base_path}/allele/beta/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/allele/beta/validation.tsv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Berechnung der Anzahl der Zeilen für jedes Set\n",
    "results = {}\n",
    "for dataset_name, paths in datasets.items():\n",
    "    # Daten laden\n",
    "    train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
    "    test_df = pd.read_csv(paths[\"test\"], sep='\\t')\n",
    "    validation_df = pd.read_csv(paths[\"validation\"], sep='\\t')\n",
    "    \n",
    "    # Anzahl der Zeilen berechnen\n",
    "    train_length = len(train_df)\n",
    "    test_length = len(test_df)\n",
    "    validation_length = len(validation_df)\n",
    "    total_length = train_length + test_length + validation_length\n",
    "    \n",
    "    # Zähle die Anzahl der Bindings 1 und 0 in jedem Datensatz\n",
    "    train_binding_counts = train_df['Binding'].value_counts()\n",
    "    test_binding_counts = test_df['Binding'].value_counts()\n",
    "    validation_binding_counts = validation_df['Binding'].value_counts()\n",
    "    \n",
    "    # Zähle die Anzahl der TPP1, TPP2, TPP3 Einträge in jedem Datensatz\n",
    "    train_task_counts = train_df['task'].value_counts()\n",
    "    test_task_counts = test_df['task'].value_counts()\n",
    "    validation_task_counts = validation_df['task'].value_counts()\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    results[dataset_name] = {\n",
    "        \"Train\": train_length,\n",
    "        \"Train_Binding_1\": train_binding_counts.get(1, 0),\n",
    "        \"Train_Binding_0\": train_binding_counts.get(0, 0),\n",
    "        \"Train_TPP1\": train_task_counts.get(\"TPP1\", 0),\n",
    "        \"Train_TPP2\": train_task_counts.get(\"TPP2\", 0),\n",
    "        \"Train_TPP3\": train_task_counts.get(\"TPP3\", 0),\n",
    "        \"Train_TPP4\": train_task_counts.get(\"TPP4\", 0),\n",
    "        \"Test\": test_length,\n",
    "        \"Test_Binding_1\": test_binding_counts.get(1, 0),\n",
    "        \"Test_Binding_0\": test_binding_counts.get(0, 0),\n",
    "        \"Test_TPP1\": test_task_counts.get(\"TPP1\", 0),\n",
    "        \"Test_TPP2\": test_task_counts.get(\"TPP2\", 0),\n",
    "        \"Test_TPP3\": test_task_counts.get(\"TPP3\", 0),\n",
    "        \"Test_TPP4\": test_task_counts.get(\"TPP4\", 0),\n",
    "        \"Validation\": validation_length,\n",
    "        \"Validation_Binding_1\": validation_binding_counts.get(1, 0),\n",
    "        \"Validation_Binding_0\": validation_binding_counts.get(0, 0),\n",
    "        \"Validation_TPP1\": validation_task_counts.get(\"TPP1\", 0),\n",
    "        \"Validation_TPP2\": validation_task_counts.get(\"TPP2\", 0),\n",
    "        \"Validation_TPP3\": validation_task_counts.get(\"TPP3\", 0),\n",
    "        \"Validation_TPP4\": validation_task_counts.get(\"TPP4\", 0),\n",
    "        \"Total\": total_length\n",
    "    }\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for dataset, lengths in results.items():\n",
    "    print(f'--- {dataset.replace(\"_\", \" \").title()} ---')\n",
    "    print(f'Anzahl der Zeilen im Trainingsdatensatz: {lengths[\"Train\"]} (Binding=1: {lengths[\"Train_Binding_1\"]}, Binding=0: {lengths[\"Train_Binding_0\"]}, TPP1: {lengths[\"Train_TPP1\"]}, TPP2: {lengths[\"Train_TPP2\"]}, TPP3: {lengths[\"Train_TPP3\"]})')\n",
    "    print(f'Anzahl der Zeilen im Testdatensatz: {lengths[\"Test\"]} (Binding=1: {lengths[\"Test_Binding_1\"]}, Binding=0: {lengths[\"Test_Binding_0\"]}, TPP1: {lengths[\"Test_TPP1\"]}, TPP2: {lengths[\"Test_TPP2\"]}, TPP3: {lengths[\"Test_TPP3\"]})')\n",
    "    print(f'Anzahl der Zeilen im Validierungsdatensatz: {lengths[\"Validation\"]} (Binding=1: {lengths[\"Validation_Binding_1\"]}, Binding=0: {lengths[\"Validation_Binding_0\"]}, TPP1: {lengths[\"Validation_TPP1\"]}, TPP2: {lengths[\"Validation_TPP2\"]}, TPP3: {lengths[\"Validation_TPP3\"]})')\n",
    "    print(f'Gesamtanzahl der Zeilen (Train + Test + Validation): {lengths[\"Total\"]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33704d51-288a-4c18-a3eb-42f78c0db4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f351dc2-bdb0-42fc-b50f-56d28b060a0d",
   "metadata": {},
   "source": [
    "### Anzahl Unique Epotope pro Files unabhängig der Binding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4143f853-5f8c-4e74-bab7-10c0019b1667",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115872/2672044272.py:5: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
      "/tmp/ipykernel_115872/2672044272.py:5: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Paired Gene ---\n",
      "Anzahl einzigartiger Epitope im Trainingsdatensatz: 694\n",
      "Anzahl einzigartiger Epitope im Testdatensatz: 760\n",
      "Anzahl einzigartiger Epitope im Validierungsdatensatz: 739\n",
      "Gesamtanzahl einzigartiger Epitope (Train + Test + Validation): 1377\n",
      "\n",
      "--- Paired Allele ---\n",
      "Anzahl einzigartiger Epitope im Trainingsdatensatz: 1182\n",
      "Anzahl einzigartiger Epitope im Testdatensatz: 564\n",
      "Anzahl einzigartiger Epitope im Validierungsdatensatz: 573\n",
      "Gesamtanzahl einzigartiger Epitope (Train + Test + Validation): 1377\n",
      "\n",
      "--- Beta Gene ---\n",
      "Anzahl einzigartiger Epitope im Trainingsdatensatz: 1105\n",
      "Anzahl einzigartiger Epitope im Testdatensatz: 1169\n",
      "Anzahl einzigartiger Epitope im Validierungsdatensatz: 1177\n",
      "Gesamtanzahl einzigartiger Epitope (Train + Test + Validation): 1890\n",
      "\n",
      "--- Beta Allele ---\n",
      "Anzahl einzigartiger Epitope im Trainingsdatensatz: 1606\n",
      "Anzahl einzigartiger Epitope im Testdatensatz: 1084\n",
      "Anzahl einzigartiger Epitope im Validierungsdatensatz: 1089\n",
      "Gesamtanzahl einzigartiger Epitope (Train + Test + Validation): 1890\n",
      "\n",
      "############### Überlappungen zw. den Datensätzen - welche Files haben gleiche Epitope ###############\n",
      "--- Paired Gene Overlaps ---\n",
      "Überlappung zwischen Train und Test: 293\n",
      "Überlappung zwischen Train und Validation: 298\n",
      "Überlappung zwischen Test und Validation: 431\n",
      "\n",
      "--- Paired Allele Overlaps ---\n",
      "Überlappung zwischen Train und Test: 445\n",
      "Überlappung zwischen Train und Validation: 448\n",
      "Überlappung zwischen Test und Validation: 318\n",
      "\n",
      "--- Beta Gene Overlaps ---\n",
      "Überlappung zwischen Train und Test: 633\n",
      "Überlappung zwischen Train und Validation: 627\n",
      "Überlappung zwischen Test und Validation: 772\n",
      "\n",
      "--- Beta Allele Overlaps ---\n",
      "Überlappung zwischen Train und Test: 890\n",
      "Überlappung zwischen Train und Validation: 889\n",
      "Überlappung zwischen Test und Validation: 728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Zusätzliche Analyse für einzigartige Epitope\n",
    "unique_epitopes = {}\n",
    "\n",
    "for dataset_name, paths in datasets.items():\n",
    "    train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
    "    test_df = pd.read_csv(paths[\"test\"], sep='\\t')\n",
    "    validation_df = pd.read_csv(paths[\"validation\"], sep='\\t')\n",
    "    \n",
    "    # Extrahieren einzigartiger Epitope\n",
    "    train_unique = set(train_df[\"Epitope\"].unique())\n",
    "    test_unique = set(test_df[\"Epitope\"].unique())\n",
    "    validation_unique = set(validation_df[\"Epitope\"].unique())\n",
    "    \n",
    "    # Gesamtanzahl einzigartiger Epitope\n",
    "    all_unique = train_unique | test_unique | validation_unique  # Vereinigung der Sets\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    unique_epitopes[dataset_name] = {\n",
    "        \"Train_Unique\": len(train_unique),\n",
    "        \"Test_Unique\": len(test_unique),\n",
    "        \"Validation_Unique\": len(validation_unique),\n",
    "        \"Total_Unique\": len(all_unique),\n",
    "        \"Train_Epitopes\": train_unique,\n",
    "        \"Test_Epitopes\": test_unique,\n",
    "        \"Validation_Epitopes\": validation_unique\n",
    "    }\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for dataset, epitopes in unique_epitopes.items():\n",
    "    print(f'--- {dataset.replace(\"_\", \" \").title()} ---')\n",
    "    print(f'Anzahl einzigartiger Epitope im Trainingsdatensatz: {epitopes[\"Train_Unique\"]}')\n",
    "    print(f'Anzahl einzigartiger Epitope im Testdatensatz: {epitopes[\"Test_Unique\"]}')\n",
    "    print(f'Anzahl einzigartiger Epitope im Validierungsdatensatz: {epitopes[\"Validation_Unique\"]}')\n",
    "    print(f'Gesamtanzahl einzigartiger Epitope (Train + Test + Validation): {epitopes[\"Total_Unique\"]}\\n')\n",
    "\n",
    "print(\"############### Überlappungen zw. den Datensätzen - welche Files haben gleiche Epitope ###############\")\n",
    "\n",
    "for dataset, epitopes in unique_epitopes.items():\n",
    "    overlap_train_test = len(epitopes[\"Train_Epitopes\"] & epitopes[\"Test_Epitopes\"])\n",
    "    overlap_train_validation = len(epitopes[\"Train_Epitopes\"] & epitopes[\"Validation_Epitopes\"])\n",
    "    overlap_test_validation = len(epitopes[\"Test_Epitopes\"] & epitopes[\"Validation_Epitopes\"])\n",
    "    \n",
    "    print(f'--- {dataset.replace(\"_\", \" \").title()} Overlaps ---')\n",
    "    print(f'Überlappung zwischen Train und Test: {overlap_train_test}')\n",
    "    print(f'Überlappung zwischen Train und Validation: {overlap_train_validation}')\n",
    "    print(f'Überlappung zwischen Test und Validation: {overlap_test_validation}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6737e2-e400-44eb-97cd-997b8f4e29e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "082e87c3-7fce-4880-8e0f-719fe6c001cb",
   "metadata": {},
   "source": [
    "### Anzahl Unique Epotope pro Files abhängig der Binding mit Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "923e9ae1-9d8a-49ef-b8e1-f3fde5c6930e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115872/3297729869.py:5: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
      "/tmp/ipykernel_115872/3297729869.py:5: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Paired Gene Unique Epitope Counts ---\n",
      "Train (Binding=0): 9, Train (Binding=1): 693\n",
      "Test (Binding=0): 9, Test (Binding=1): 758\n",
      "Validation (Binding=0): 9, Validation (Binding=1): 736\n",
      "\n",
      "--- Paired Allele Unique Epitope Counts ---\n",
      "Train (Binding=0): 9, Train (Binding=1): 1181\n",
      "Test (Binding=0): 9, Test (Binding=1): 562\n",
      "Validation (Binding=0): 9, Validation (Binding=1): 571\n",
      "\n",
      "--- Beta Gene Unique Epitope Counts ---\n",
      "Train (Binding=0): 9, Train (Binding=1): 1103\n",
      "Test (Binding=0): 9, Test (Binding=1): 1168\n",
      "Validation (Binding=0): 9, Validation (Binding=1): 1176\n",
      "\n",
      "--- Beta Allele Unique Epitope Counts ---\n",
      "Train (Binding=0): 9, Train (Binding=1): 1604\n",
      "Test (Binding=0): 9, Test (Binding=1): 1083\n",
      "Validation (Binding=0): 9, Validation (Binding=1): 1088\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Anzahl der einzigartigen Epitope pro Binding und pro Datei\n",
    "unique_epitopes_count = {}\n",
    "\n",
    "for dataset, paths in datasets.items():\n",
    "    train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
    "    test_df = pd.read_csv(paths[\"test\"], sep='\\t')\n",
    "    validation_df = pd.read_csv(paths[\"validation\"], sep='\\t')\n",
    "    \n",
    "    # Einzigartige Epitope für Binding=0 und Binding=1\n",
    "    train_unique = {\n",
    "        0: len(set(train_df[train_df['Binding'] == 0]['Epitope'])),\n",
    "        1: len(set(train_df[train_df['Binding'] == 1]['Epitope']))\n",
    "    }\n",
    "    test_unique = {\n",
    "        0: len(set(test_df[test_df['Binding'] == 0]['Epitope'])),\n",
    "        1: len(set(test_df[test_df['Binding'] == 1]['Epitope']))\n",
    "    }\n",
    "    validation_unique = {\n",
    "        0: len(set(validation_df[validation_df['Binding'] == 0]['Epitope'])),\n",
    "        1: len(set(validation_df[validation_df['Binding'] == 1]['Epitope']))\n",
    "    }\n",
    "    \n",
    "    # Ergebnisse speichern\n",
    "    unique_epitopes_count[dataset] = {\n",
    "        \"Train_Binding_0\": train_unique[0],\n",
    "        \"Train_Binding_1\": train_unique[1],\n",
    "        \"Test_Binding_0\": test_unique[0],\n",
    "        \"Test_Binding_1\": test_unique[1],\n",
    "        \"Validation_Binding_0\": validation_unique[0],\n",
    "        \"Validation_Binding_1\": validation_unique[1]\n",
    "    }\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "for dataset, counts in unique_epitopes_count.items():\n",
    "    print(f'--- {dataset.replace(\"_\", \" \").title()} Unique Epitope Counts ---')\n",
    "    print(f'Train (Binding=0): {counts[\"Train_Binding_0\"]}, Train (Binding=1): {counts[\"Train_Binding_1\"]}')\n",
    "    print(f'Test (Binding=0): {counts[\"Test_Binding_0\"]}, Test (Binding=1): {counts[\"Test_Binding_1\"]}')\n",
    "    print(f'Validation (Binding=0): {counts[\"Validation_Binding_0\"]}, Validation (Binding=1): {counts[\"Validation_Binding_1\"]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d7e2a13-b30e-4157-9923-161e1378499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Paired Gene Overlaps (Binding=0) ---\n",
      "Überlappung zwischen Train und Test: 9\n",
      "Überlappung zwischen Train und Validation: 9\n",
      "Überlappung zwischen Test und Validation: 9\n",
      "\n",
      "--- Paired Gene Overlaps (Binding=1) ---\n",
      "Überlappung zwischen Train und Test: 290\n",
      "Überlappung zwischen Train und Validation: 295\n",
      "Überlappung zwischen Test und Validation: 427\n",
      "\n",
      "--- Paired Allele Overlaps (Binding=0) ---\n",
      "Überlappung zwischen Train und Test: 9\n",
      "Überlappung zwischen Train und Validation: 9\n",
      "Überlappung zwischen Test und Validation: 9\n",
      "\n",
      "--- Paired Allele Overlaps (Binding=1) ---\n",
      "Überlappung zwischen Train und Test: 443\n",
      "Überlappung zwischen Train und Validation: 445\n",
      "Überlappung zwischen Test und Validation: 315\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115872/719111292.py:2: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Beta Gene Overlaps (Binding=0) ---\n",
      "Überlappung zwischen Train und Test: 9\n",
      "Überlappung zwischen Train und Validation: 9\n",
      "Überlappung zwischen Test und Validation: 9\n",
      "\n",
      "--- Beta Gene Overlaps (Binding=1) ---\n",
      "Überlappung zwischen Train und Test: 631\n",
      "Überlappung zwischen Train und Validation: 625\n",
      "Überlappung zwischen Test und Validation: 770\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115872/719111292.py:2: DtypeWarning: Columns (0,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Beta Allele Overlaps (Binding=0) ---\n",
      "Überlappung zwischen Train und Test: 9\n",
      "Überlappung zwischen Train und Validation: 9\n",
      "Überlappung zwischen Test und Validation: 9\n",
      "\n",
      "--- Beta Allele Overlaps (Binding=1) ---\n",
      "Überlappung zwischen Train und Test: 888\n",
      "Überlappung zwischen Train und Validation: 887\n",
      "Überlappung zwischen Test und Validation: 726\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset, paths in datasets.items():\n",
    "    train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
    "    test_df = pd.read_csv(paths[\"test\"], sep='\\t')\n",
    "    validation_df = pd.read_csv(paths[\"validation\"], sep='\\t')\n",
    "    \n",
    "    # Aufteilen der Epitope nach Binding-Wert (0 oder 1)\n",
    "    train_epitopes_binding = {\n",
    "        0: set(train_df[train_df['Binding'] == 0]['Epitope']),\n",
    "        1: set(train_df[train_df['Binding'] == 1]['Epitope'])\n",
    "    }\n",
    "    test_epitopes_binding = {\n",
    "        0: set(test_df[test_df['Binding'] == 0]['Epitope']),\n",
    "        1: set(test_df[test_df['Binding'] == 1]['Epitope'])\n",
    "    }\n",
    "    validation_epitopes_binding = {\n",
    "        0: set(validation_df[validation_df['Binding'] == 0]['Epitope']),\n",
    "        1: set(validation_df[validation_df['Binding'] == 1]['Epitope'])\n",
    "    }\n",
    "    \n",
    "    # Berechnung der Überlappung pro Binding-Wert\n",
    "    for binding in [0, 1]:\n",
    "        overlap_train_test = len(train_epitopes_binding[binding] & test_epitopes_binding[binding])\n",
    "        overlap_train_validation = len(train_epitopes_binding[binding] & validation_epitopes_binding[binding])\n",
    "        overlap_test_validation = len(test_epitopes_binding[binding] & validation_epitopes_binding[binding])\n",
    "        \n",
    "        print(f'--- {dataset.replace(\"_\", \" \").title()} Overlaps (Binding={binding}) ---')\n",
    "        print(f'Überlappung zwischen Train und Test: {overlap_train_test}')\n",
    "        print(f'Überlappung zwischen Train und Validation: {overlap_train_validation}')\n",
    "        print(f'Überlappung zwischen Test und Validation: {overlap_test_validation}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6319fa00-ae4c-4c9b-a523-58e98f3d7c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0d2c6b0-c059-4da1-9913-e0a168a84a95",
   "metadata": {},
   "source": [
    "### Anzahl Unique Epotope pro Files abhängig der Binding mit Mixed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83c3cb8f-d455-458c-aaf6-8f1025963dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Paired Gene Unique Epitope Counts ---\n",
      "Train (Binding=0): 607, Train (Binding=1): 693\n",
      "Test (Binding=0): 374, Test (Binding=1): 758\n",
      "Validation (Binding=0): 383, Validation (Binding=1): 736\n",
      "\n",
      "--- Paired Allele Unique Epitope Counts ---\n",
      "Train (Binding=0): 622, Train (Binding=1): 1181\n",
      "Test (Binding=0): 374, Test (Binding=1): 562\n",
      "Validation (Binding=0): 383, Validation (Binding=1): 571\n",
      "\n",
      "--- Beta Gene Unique Epitope Counts ---\n",
      "Train (Binding=0): 947, Train (Binding=1): 1103\n",
      "Test (Binding=0): 779, Test (Binding=1): 1168\n",
      "Validation (Binding=0): 799, Validation (Binding=1): 1176\n",
      "\n",
      "--- Beta Allele Unique Epitope Counts ---\n",
      "Train (Binding=0): 975, Train (Binding=1): 1604\n",
      "Test (Binding=0): 779, Test (Binding=1): 1083\n",
      "Validation (Binding=0): 799, Validation (Binding=1): 1088\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Anzahl der einzigartigen Epitope pro Binding und pro Datei\n",
    "unique_epitopes_count = {}\n",
    "\n",
    "for dataset, paths in datasets.items():\n",
    "    train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
    "    test_df = pd.read_csv(paths[\"test\"], sep='\\t')\n",
    "    validation_df = pd.read_csv(paths[\"validation\"], sep='\\t')\n",
    "    \n",
    "    # Einzigartige Epitope für Binding=0 und Binding=1\n",
    "    train_unique = {\n",
    "        0: len(set(train_df[train_df['Binding'] == 0]['Epitope'])),\n",
    "        1: len(set(train_df[train_df['Binding'] == 1]['Epitope']))\n",
    "    }\n",
    "    test_unique = {\n",
    "        0: len(set(test_df[test_df['Binding'] == 0]['Epitope'])),\n",
    "        1: len(set(test_df[test_df['Binding'] == 1]['Epitope']))\n",
    "    }\n",
    "    validation_unique = {\n",
    "        0: len(set(validation_df[validation_df['Binding'] == 0]['Epitope'])),\n",
    "        1: len(set(validation_df[validation_df['Binding'] == 1]['Epitope']))\n",
    "    }\n",
    "    \n",
    "    # Ergebnisse speichern\n",
    "    unique_epitopes_count[dataset] = {\n",
    "        \"Train_Binding_0\": train_unique[0],\n",
    "        \"Train_Binding_1\": train_unique[1],\n",
    "        \"Test_Binding_0\": test_unique[0],\n",
    "        \"Test_Binding_1\": test_unique[1],\n",
    "        \"Validation_Binding_0\": validation_unique[0],\n",
    "        \"Validation_Binding_1\": validation_unique[1]\n",
    "    }\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "for dataset, counts in unique_epitopes_count.items():\n",
    "    print(f'--- {dataset.replace(\"_\", \" \").title()} Unique Epitope Counts ---')\n",
    "    print(f'Train (Binding=0): {counts[\"Train_Binding_0\"]}, Train (Binding=1): {counts[\"Train_Binding_1\"]}')\n",
    "    print(f'Test (Binding=0): {counts[\"Test_Binding_0\"]}, Test (Binding=1): {counts[\"Test_Binding_1\"]}')\n",
    "    print(f'Validation (Binding=0): {counts[\"Validation_Binding_0\"]}, Validation (Binding=1): {counts[\"Validation_Binding_1\"]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b02cf5a-8380-40ea-b1c6-effbdc9cd2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51db4315-f255-469b-b357-180fb1314416",
   "metadata": {},
   "source": [
    "### Anzahl Unique Epotope pro Files abhängig der Binding mit allen Rows von 10X und BA aufgefüllt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fa304e1-11da-4adc-af44-eeeec009850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Paired Gene Unique Epitope Counts ---\n",
      "Train (Binding=0): 607, Train (Binding=1): 693\n",
      "Test (Binding=0): 374, Test (Binding=1): 758\n",
      "Validation (Binding=0): 383, Validation (Binding=1): 736\n",
      "\n",
      "--- Paired Allele Unique Epitope Counts ---\n",
      "Train (Binding=0): 622, Train (Binding=1): 1181\n",
      "Test (Binding=0): 374, Test (Binding=1): 562\n",
      "Validation (Binding=0): 383, Validation (Binding=1): 571\n",
      "\n",
      "--- Beta Gene Unique Epitope Counts ---\n",
      "Train (Binding=0): 947, Train (Binding=1): 1103\n",
      "Test (Binding=0): 779, Test (Binding=1): 1168\n",
      "Validation (Binding=0): 799, Validation (Binding=1): 1176\n",
      "\n",
      "--- Beta Allele Unique Epitope Counts ---\n",
      "Train (Binding=0): 986, Train (Binding=1): 1604\n",
      "Test (Binding=0): 787, Test (Binding=1): 1083\n",
      "Validation (Binding=0): 810, Validation (Binding=1): 1088\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Beispielpfade für Train-, Test-, und Validierungsdatensätze für alle vier Kategorien\n",
    "base_path = 'data_10x/splitted_datasets'\n",
    "precision = 'allele' #allele or gene\n",
    "\n",
    "# Definierte Pfade für alle vier Kategorien\n",
    "datasets = {\n",
    "    \"paired_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/paired/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/paired/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/paired/validation.tsv\"\n",
    "    },\n",
    "    \"paired_allele\": {\n",
    "        \"train\": f\"{base_path}/allele/paired/train.tsv\",\n",
    "        \"test\": f\"{base_path}/allele/paired/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/allele/paired/validation.tsv\"\n",
    "    },\n",
    "    \"beta_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/beta/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/beta/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/beta/validation.tsv\"\n",
    "    },\n",
    "    \"beta_allele\": {\n",
    "        \"train\": f\"{base_path}/allele/beta/train.tsv\",\n",
    "        \"test\": f\"{base_path}/allele/beta/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/allele/beta/validation.tsv\"\n",
    "    }\n",
    "}\n",
    "# Anzahl der einzigartigen Epitope pro Binding und pro Datei\n",
    "unique_epitopes_count = {}\n",
    "\n",
    "for dataset, paths in datasets.items():\n",
    "    train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
    "    test_df = pd.read_csv(paths[\"test\"], sep='\\t')\n",
    "    validation_df = pd.read_csv(paths[\"validation\"], sep='\\t')\n",
    "    \n",
    "    # Einzigartige Epitope für Binding=0 und Binding=1\n",
    "    train_unique = {\n",
    "        0: len(set(train_df[train_df['Binding'] == 0]['Epitope'])),\n",
    "        1: len(set(train_df[train_df['Binding'] == 1]['Epitope']))\n",
    "    }\n",
    "    test_unique = {\n",
    "        0: len(set(test_df[test_df['Binding'] == 0]['Epitope'])),\n",
    "        1: len(set(test_df[test_df['Binding'] == 1]['Epitope']))\n",
    "    }\n",
    "    validation_unique = {\n",
    "        0: len(set(validation_df[validation_df['Binding'] == 0]['Epitope'])),\n",
    "        1: len(set(validation_df[validation_df['Binding'] == 1]['Epitope']))\n",
    "    }\n",
    "    \n",
    "    # Ergebnisse speichern\n",
    "    unique_epitopes_count[dataset] = {\n",
    "        \"Train_Binding_0\": train_unique[0],\n",
    "        \"Train_Binding_1\": train_unique[1],\n",
    "        \"Test_Binding_0\": test_unique[0],\n",
    "        \"Test_Binding_1\": test_unique[1],\n",
    "        \"Validation_Binding_0\": validation_unique[0],\n",
    "        \"Validation_Binding_1\": validation_unique[1]\n",
    "    }\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "for dataset, counts in unique_epitopes_count.items():\n",
    "    print(f'--- {dataset.replace(\"_\", \" \").title()} Unique Epitope Counts ---')\n",
    "    print(f'Train (Binding=0): {counts[\"Train_Binding_0\"]}, Train (Binding=1): {counts[\"Train_Binding_1\"]}')\n",
    "    print(f'Test (Binding=0): {counts[\"Test_Binding_0\"]}, Test (Binding=1): {counts[\"Test_Binding_1\"]}')\n",
    "    print(f'Validation (Binding=0): {counts[\"Validation_Binding_0\"]}, Validation (Binding=1): {counts[\"Validation_Binding_1\"]}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c10a929-1757-468d-9ea8-07fc50901470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d97098dd-5647-477f-8693-156391a66b3f",
   "metadata": {},
   "source": [
    "### Anzahl TPPs in Mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b136c9a8-171d-48a8-a5ce-18f3583f0b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene\n",
      "Train data:\n",
      "task\n",
      "TPP1    61498\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation data:\n",
      "task\n",
      "TPP1    36934\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test data:\n",
      "task\n",
      "TPP1    24419\n",
      "TPP2    12164\n",
      "TPP4      136\n",
      "TPP3       87\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "precision = \"gene\"\n",
    "\n",
    "splitted_data_path = f\"data_10x/splitted_datasets/{precision}/paired\"  # paired oder beta\n",
    "\n",
    "train_file_name = 'train.tsv'\n",
    "test_file_name = 'test.tsv'\n",
    "validation_file_name = 'validation.tsv'\n",
    "\n",
    "train_file = f\"{splitted_data_path}/{train_file_name}\"\n",
    "test_file = f\"{splitted_data_path}/{test_file_name}\"\n",
    "validation_file = f\"{splitted_data_path}/{validation_file_name}\"\n",
    "\n",
    "# Dateien einlesen\n",
    "df_train = pd.read_csv(train_file, sep=\"\\t\")\n",
    "df_test = pd.read_csv(test_file, sep=\"\\t\")\n",
    "df_validate = pd.read_csv(validation_file, sep=\"\\t\")\n",
    "\n",
    "# Sicherstellen, dass die notwendigen Spalten vorhanden sind\n",
    "required_columns = ['TRA_CDR3', 'TRB_CDR3', 'Epitope']\n",
    "for name, df in [(\"Train\", df_train), (\"Validate\", df_validate), (\"Test\", df_test)]:\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Fehlende Spalten in {name}-DataFrame: {missing_columns}\")\n",
    "        raise ValueError(f\"DataFrame {name} hat nicht alle benötigten Spalten!\")\n",
    "\n",
    "# Training und Validation kombinieren\n",
    "df_train_validate = pd.concat([df_train, df_validate])\n",
    "\n",
    "# CDR3- und Epitope-Schlüssel erstellen\n",
    "tcr_key = \"tcr_key\"\n",
    "df_train_validate[tcr_key] = df_train_validate['TRA_CDR3'].astype(str) + '_' + df_train_validate['TRB_CDR3'].astype(str)\n",
    "df_test[tcr_key] = df_test['TRA_CDR3'].astype(str) + '_' + df_test['TRB_CDR3'].astype(str)\n",
    "df_train[tcr_key] = df_train['TRA_CDR3'].astype(str) + '_' + df_train['TRB_CDR3'].astype(str)\n",
    "df_validate[tcr_key] = df_validate['TRA_CDR3'].astype(str) + '_' + df_validate['TRB_CDR3'].astype(str)\n",
    "\n",
    "# Sets für Lookup erstellen\n",
    "epitopes_in_train = set(df_train_validate['Epitope'])\n",
    "cdr3_in_train = set(df_train_validate[tcr_key])\n",
    "\n",
    "# Funktion zur Klassifizierung\n",
    "def classify_task(row, epitopes_in_train, cdr3_in_train):\n",
    "    epitope_exists = row['Epitope'] in epitopes_in_train\n",
    "    cdr3_exists = row[tcr_key] in cdr3_in_train\n",
    "    \n",
    "    if epitope_exists and cdr3_exists:\n",
    "        return 'TPP1'\n",
    "    elif epitope_exists and not cdr3_exists:\n",
    "        return 'TPP2'\n",
    "    elif not epitope_exists and not cdr3_exists:\n",
    "        return 'TPP3'\n",
    "    elif not epitope_exists and cdr3_exists:\n",
    "        return 'TPP4'\n",
    "\n",
    "# Klassifizierung auf alle Datensätze anwenden\n",
    "df_train['task'] = df_train.apply(classify_task, axis=1, args=(epitopes_in_train, cdr3_in_train))\n",
    "df_validate['task'] = df_validate.apply(classify_task, axis=1, args=(epitopes_in_train, cdr3_in_train))\n",
    "df_test['task'] = df_test.apply(classify_task, axis=1, args=(epitopes_in_train, cdr3_in_train))\n",
    "\n",
    "# Anzahl der Tasks berechnen\n",
    "task_counts_train = df_train['task'].value_counts()\n",
    "task_counts_validate = df_validate['task'].value_counts()\n",
    "task_counts_test = df_test['task'].value_counts()\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print(precision)\n",
    "print(\"Train data:\")\n",
    "print(task_counts_train)\n",
    "print(\"\\nValidation data:\")\n",
    "print(task_counts_validate)\n",
    "print(\"\\nTest data:\")\n",
    "print(task_counts_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d2b5830-f364-4b80-b4b1-692edb881ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BA  gene\n",
      "Train data:\n",
      "task\n",
      "TPP1    66958\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Validation data:\n",
      "task\n",
      "TPP1    14423\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test data:\n",
      "task\n",
      "TPP2    7816\n",
      "TPP1    5879\n",
      "TPP3     546\n",
      "TPP4     174\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "precision = \"gene\"\n",
    "pipeline_data = './../../BA/BA_ZHAW/data'\n",
    "pipeline_data_splitted = f'{pipeline_data}/splitted_datasets'\n",
    "\n",
    "splitted_data_path = f\"{pipeline_data_splitted}/{precision}/paired\"  # paired oder beta\n",
    "\n",
    "train_file_name = 'train.tsv'\n",
    "test_file_name = 'test.tsv'\n",
    "validation_file_name = 'validation.tsv'\n",
    "\n",
    "train_file = f\"{splitted_data_path}/{train_file_name}\"\n",
    "test_file = f\"{splitted_data_path}/{test_file_name}\"\n",
    "validation_file = f\"{splitted_data_path}/{validation_file_name}\"\n",
    "\n",
    "# Dateien einlesen\n",
    "df_train = pd.read_csv(train_file, sep=\"\\t\")\n",
    "df_test = pd.read_csv(test_file, sep=\"\\t\")\n",
    "df_validate = pd.read_csv(validation_file, sep=\"\\t\")\n",
    "\n",
    "# Sicherstellen, dass die notwendigen Spalten vorhanden sind\n",
    "required_columns = ['TRA_CDR3', 'TRB_CDR3', 'Epitope']\n",
    "for name, df in [(\"Train\", df_train), (\"Validate\", df_validate), (\"Test\", df_test)]:\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        print(f\"Fehlende Spalten in {name}-DataFrame: {missing_columns}\")\n",
    "        raise ValueError(f\"DataFrame {name} hat nicht alle benötigten Spalten!\")\n",
    "\n",
    "# Training und Validation kombinieren\n",
    "df_train_validate = pd.concat([df_train, df_validate])\n",
    "\n",
    "# CDR3- und Epitope-Schlüssel erstellen\n",
    "tcr_key = \"tcr_key\"\n",
    "df_train_validate[tcr_key] = df_train_validate['TRA_CDR3'].astype(str) + '_' + df_train_validate['TRB_CDR3'].astype(str)\n",
    "df_test[tcr_key] = df_test['TRA_CDR3'].astype(str) + '_' + df_test['TRB_CDR3'].astype(str)\n",
    "df_train[tcr_key] = df_train['TRA_CDR3'].astype(str) + '_' + df_train['TRB_CDR3'].astype(str)\n",
    "df_validate[tcr_key] = df_validate['TRA_CDR3'].astype(str) + '_' + df_validate['TRB_CDR3'].astype(str)\n",
    "\n",
    "# Sets für Lookup erstellen\n",
    "epitopes_in_train = set(df_train_validate['Epitope'])\n",
    "cdr3_in_train = set(df_train_validate[tcr_key])\n",
    "\n",
    "# Funktion zur Klassifizierung\n",
    "def classify_task(row, epitopes_in_train, cdr3_in_train):\n",
    "    epitope_exists = row['Epitope'] in epitopes_in_train\n",
    "    cdr3_exists = row[tcr_key] in cdr3_in_train\n",
    "    \n",
    "    if epitope_exists and cdr3_exists:\n",
    "        return 'TPP1'\n",
    "    elif epitope_exists and not cdr3_exists:\n",
    "        return 'TPP2'\n",
    "    elif not epitope_exists and not cdr3_exists:\n",
    "        return 'TPP3'\n",
    "    elif not epitope_exists and cdr3_exists:\n",
    "        return 'TPP4'\n",
    "\n",
    "# Klassifizierung auf alle Datensätze anwenden\n",
    "df_train['task'] = df_train.apply(classify_task, axis=1, args=(epitopes_in_train, cdr3_in_train))\n",
    "df_validate['task'] = df_validate.apply(classify_task, axis=1, args=(epitopes_in_train, cdr3_in_train))\n",
    "df_test['task'] = df_test.apply(classify_task, axis=1, args=(epitopes_in_train, cdr3_in_train))\n",
    "\n",
    "# Anzahl der Tasks berechnen\n",
    "task_counts_train = df_train['task'].value_counts()\n",
    "task_counts_validate = df_validate['task'].value_counts()\n",
    "task_counts_test = df_test['task'].value_counts()\n",
    "\n",
    "# Ergebnisse ausgeben\n",
    "print(\"BA \", precision)\n",
    "print(\"Train data:\")\n",
    "print(task_counts_train)\n",
    "print(\"\\nValidation data:\")\n",
    "print(task_counts_validate)\n",
    "print(\"\\nTest data:\")\n",
    "print(task_counts_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
