{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set precision of mhc and V/J values (gene or allele)\n",
    "precision = 'gene'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function is not thread safe\n",
    "def create_folders_if_not_exists(folders):\n",
    "  for path in folders:\n",
    "    if not os.path.exists(path):\n",
    "      os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_data = './data'\n",
    "pipeline_data_plain = f'{pipeline_data}/plain_datasets'\n",
    "pipeline_data_cleaned = f'{pipeline_data}/cleaned_datasets'\n",
    "pipeline_data_concatenated = f'{pipeline_data}/concatenated_datasets'\n",
    "pipeline_data_splitted = f'{pipeline_data}/splitted_datasets'\n",
    "pipeline_data_temp_bucket = f'{pipeline_data}/temp'\n",
    "\n",
    "pipeline_folders = [pipeline_data, pipeline_data_plain, pipeline_data_cleaned, pipeline_data_concatenated, pipeline_data_splitted, pipeline_data_temp_bucket]\n",
    "\n",
    "create_folders_if_not_exists(pipeline_folders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IEDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directories\n",
    "IEDB_data_plain = f'{pipeline_data_plain}/IEDB'\n",
    "IEDB_data_cleaned = f'{pipeline_data_cleaned}/IEDB'\n",
    "IEDB_data_fitted = f'{pipeline_data_temp_bucket}/IEDB'\n",
    "\n",
    "IEDB_folders = [IEDB_data_plain, IEDB_data_cleaned, IEDB_data_fitted]\n",
    "create_folders_if_not_exists(IEDB_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook IEDB fit data\n",
    "path_prefix_plain = IEDB_data_plain\n",
    "path_prefix_fitted = IEDB_data_fitted\n",
    "mhc_I_input_beta = f\"{path_prefix_plain}/MHCI_IEDB_beta_export.csv\"\n",
    "mhc_I_output_beta = f\"{path_prefix_fitted}/IEDB_beta_fitted.csv\"\n",
    "mhc_I_input_paired = f\"{path_prefix_plain}/MHCI_IEDB_paired_export.csv\"\n",
    "mhc_I_output_paired = f\"{path_prefix_fitted}/IEDB_paired_fitted.csv\"\n",
    "\n",
    "# fit IEDB data\n",
    "#%run ./data_scripts/IEDB/IEDB_fitted_dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook IEDB clean data\n",
    "path_prefix_fitted = IEDB_data_fitted\n",
    "path_prefix_cleaned =  IEDB_data_cleaned\n",
    "fitted_file_beta = \"IEDB_beta_fitted.csv\"\n",
    "fitted_file_paired = \"IEDB_paired_fitted.csv\"\n",
    "cleaned_file_beta = \"IEDB_cleaned_data_beta.csv\"\n",
    "cleaned_file_paired = \"IEDB_cleaned_data_paired.csv\"\n",
    "\n",
    "# clean IEDB data\n",
    "#%run ./data_scripts/IEDB/IEDB_clean_dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "IEDB_cleaned_beta_output = f'{IEDB_data_cleaned}/{cleaned_file_beta}'\n",
    "IEDB_cleaned_paired_output = f'{IEDB_data_cleaned}/{cleaned_file_paired}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McPAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directories\n",
    "McPas_data_plain = f'{pipeline_data_plain}/McPas'\n",
    "McPas_data_cleaned = f'{pipeline_data_cleaned}/McPas'\n",
    "McPas_data_fitted = f'{pipeline_data_temp_bucket}/McPas'\n",
    "\n",
    "McPas_folders = [McPas_data_plain, McPas_data_cleaned, McPas_data_fitted]\n",
    "create_folders_if_not_exists(McPas_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook McPAS fit data\n",
    "input_file = f'{McPas_data_plain}/McPAS-TCR.csv'\n",
    "path_prefix_fitted = McPas_data_fitted\n",
    "fitted_file = 'McPAS_fitted.tsv'\n",
    "\n",
    "# fit McPAS data\n",
    "%run ./data_scripts/McPas-TCR/fit_data_mcpastcr_both.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHC Class I has 10078 entries\n",
      "whole dataframe has 13701 entries\n",
      "filtered to only use MHC Class I. Length of dataset: 10078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134852/2652240660.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  mcpastcr_cleaned_both_df = mcpastcr_cleaned_both_df[~mask]\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for notebook McPAS clean data\n",
    "fitted_input_file = f'{McPas_data_fitted}/{fitted_file}'\n",
    "path_prefix_cleaned = McPas_data_cleaned\n",
    "cleaned_file_paired = 'McPAS_cleaned_data_paired.tsv'\n",
    "cleaned_file_beta = 'McPAS_cleaned_data_beta.tsv'\n",
    "\n",
    "# clean McPAS data\n",
    "%run ./data_scripts/McPas-TCR/clean_data_mcpastcr_both.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "McPAS_cleaned_beta_output = f'{McPas_data_cleaned}/{cleaned_file_beta}'\n",
    "McPAS_cleaned_paired_output = f'{McPas_data_cleaned}/{cleaned_file_paired}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VDJdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare directories\n",
    "VDJdb_data_plain = f'{pipeline_data_plain}/VDJdb'\n",
    "VDJdb_data_cleaned = f'{pipeline_data_cleaned}/VDJdb'\n",
    "VDJdb_data_fitted = f'{pipeline_data_temp_bucket}/VDJdb'\n",
    "\n",
    "VDJdb_folders = [VDJdb_data_plain, VDJdb_data_cleaned, VDJdb_data_fitted]\n",
    "create_folders_if_not_exists(VDJdb_folders)\n",
    "\n",
    "fitted_beta_file = 'VDJdb_beta_fitted.tsv'\n",
    "fitted_paired_file = 'VDJdb_paired_fitted.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook VDJdb fit data paired\n",
    "input_file = f'{VDJdb_data_plain}/VDJdb_paired_only.tsv'\n",
    "path_prefix_fitted = VDJdb_data_fitted\n",
    "fitted_file = fitted_paired_file\n",
    "\n",
    "# fit paired VDJdb data\n",
    "%run ./data_scripts/VDJdb/fit_data_vdjdb_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare parameters for notebook VDJdb fit data beta\n",
    "input_file = f'{VDJdb_data_plain}/VDJdb_beta_only.tsv'\n",
    "path_prefix_fitted = VDJdb_data_fitted\n",
    "fitted_file = fitted_beta_file\n",
    "\n",
    "# fit beta VDJdb data\n",
    "%run ./data_scripts/VDJdb/fit_data_vdjdb_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHC Class I has 27414 entries\n",
      "whole dataframe has 28119 entries\n",
      "filtered to only use MHC Class I. Length of dataset: 27414\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for notebook VDJdb clean data paired\n",
    "input_file = f'{VDJdb_data_fitted}/{fitted_paired_file}'\n",
    "cleaned_file_paired = 'VDJdb_cleaned_data_paired.tsv'\n",
    "output_file = f'{VDJdb_data_cleaned}/{cleaned_file_paired}'\n",
    "\n",
    "# clean paired VDJdb data\n",
    "%run ./data_scripts/VDJdb/clean_data_vdjdb_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHC Class I has 46507 entries\n",
      "whole dataframe has 49042 entries\n",
      "filtered to only use MHC Class I. Length of dataset: 46507\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for notebook VDJdb clean data beta\n",
    "input_file = f'{VDJdb_data_fitted}/{fitted_beta_file}'\n",
    "cleaned_file_beta = 'VDJdb_cleaned_data_beta.tsv'\n",
    "output_file = f'{VDJdb_data_cleaned}/{cleaned_file_beta}'\n",
    "\n",
    "# clean beta VDJdb data\n",
    "%run ./data_scripts/VDJdb/clean_data_vdjdb_beta.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "VDJdb_cleaned_beta_output = f'{VDJdb_data_cleaned}/{cleaned_file_beta}'\n",
    "VDJdb_cleaned_paired_output = f'{VDJdb_data_cleaned}/{cleaned_file_paired}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Concatenation\n",
    "The concatenation includes further cleaning and advanced removal of duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of beta_df: 231627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"H-2Kb\" for species homosapiens: unrecognised gene name. (best attempted fix: \"HLA-H-2KB\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*24:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-B*12\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*08:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA class I\" for species homosapiens: unrecognised gene name. (best attempted fix: \"HLA-HLACLASSI\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV9-02\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV9-2\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV2-7*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV19*04\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV26-2*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV19*05\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV26-1*03\" for species homosapiens: nonexistent allele for recognised gene. (best attempted fix: \"TRBV26*03\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV2-6*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV8-4*05\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV2-2*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV1-4*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV2-03\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV2-3\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV2-05\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV2-5\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV13-2\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV13-06\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV13-6\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV1-05\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV1-5\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV21-03\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV21-3\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV6-2*02\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRB17-1\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRB17-1\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBV21-3\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV21-3\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBV13-6\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV13-6\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBV13-3\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV13-3\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRVB06\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRVB6\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRVB6\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRVB6\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRVB12\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRVB12\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBV12-3/4\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV12-3/4\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBB27\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBB27\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"05-07*01\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TR5-7*01\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ5-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ10-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ20-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ19-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ17-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ38-2*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"Donor 13\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRDON/OR13\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"Negative\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRNEGATIVE\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ37*02\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ8*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ52*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ9*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ33*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ36*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ53*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ54*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ3-1*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ42*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ39*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ45*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ31*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ43*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ1-7\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ5-6\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ3-2\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize Donor13: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CALQDXNTGEXFF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CSARTGDRTEAFX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CASSILGWSEAFX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CASSLRTRTDTQYX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following script removes a lot of rows. They are kept and some of them get added again later\n",
      "distinct entries (all columns, keep=first). 36836 entries removed.\n",
      "removed all duplicates (CDR3, Epitope) from distinct values (most_important_columns, keep=False). 47581 entries removed.\n",
      "beta removed entries df length: 47581\n",
      "\n",
      "\n",
      "Number of groups formed: 18337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134852/3398904298.py:24: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  duplicates_to_add = pd.concat([duplicates_to_add, group[group['is_duplicated'] == False]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32617 can be re-added to the no-duplicated dataframe\n",
      "from the plain dataset which has 231622 entries, 51800 entries have been removed.\n",
      "for beta dataset :\n",
      "size difference is: 51800\n",
      "  179822 information score cleaned: 5.202255563835348\n",
      "  231622 information score dropout: 5.0485964200291855\n",
      "final_beta_df length = 179822\n",
      "length of paired_df: 54338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"H-2Kb\" for species homosapiens: unrecognised gene name. (best attempted fix: \"HLA-H-2KB\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*24:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-A*08:01\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA-B*12\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"HLA class I\" for species homosapiens: unrecognised gene name. (best attempted fix: \"HLA-HLACLASSI\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV1-4\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV2-2\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV251\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV21-2\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRDAV1*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRA21-01\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRA21-1\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV19-*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV1-4\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRAV1-4\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV13/1*02 (F)\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRAV13/DV1*02\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV2-2\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRAV2-2\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV9*02\" for species homosapiens: nonexistent allele for recognised gene. (best attempted fix: \"TRAV9-1*02\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAV12-4\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV24*0\" for species homosapiens: nonexistent allele for recognised gene. (best attempted fix: \"TRAV24*00\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRAV12-4\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRAV12-4\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBV9-02\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV9-2\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TCRBV13-3\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRBV13-3\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"CATSESSGQTYEQYF\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRCAT-E--GQTYEQYF\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRAJ53*02\" for species homosapiens: nonexistent allele for recognised gene.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"Negative\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRNEGATIVE\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"Donor 13\" for species homosapiens: unrecognised gene name. (best attempted fix: \"TRDON/OR13\").\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ5-6\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ5-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ10-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ20-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ38-2*01\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ17-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/_utils/warnings.py:12: UserWarning: Failed to standardize \"TRBJ19-1\" for species homosapiens: unrecognised gene name.\n",
      "  warn(warning_message)\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize TRAJ37: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize XVXNREDKLVF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CVVNNNXDMRF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CVVNGMDSSYKLXF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize VQQDLGY##GGTSYGKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize VL*#GGGADGLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVED#ISSGSARQLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CIVRQLGTGAVVTIN*#F: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CTSVQQAL#GGSQGNLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CIVSVLG#SGGSNYKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize XEXXGGXXRXXX##: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CI##GGADGLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CTPG#SQGNLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CIVRV#NSNSGYALNF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVSA#KAAGNKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVSAAX#AGNKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVSGPS#TDSWGKLQF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAPQTRRLATTVS*#W: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAGLGGT#GGSNYKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVND#NTDKLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize VL*VLPG#ALNF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAIELQAR##: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize VQQAQ#YGGSQGNLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAASSWS*#GLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVEDRGEKHXSCL: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVERGL##GSQGNLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVS#SNFGNEKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CALPP#YNFNKFYF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAXSGGGSYQLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAG#YGNKLVF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CIVRVA#SGNTPLVF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CIXXL*NDIGENMFLI: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CISP*SGSSNTGKLIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize VQQAG#SNYKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CATLCPQ#NKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAASGXGTYKYIF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAXRGGSEKLVF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CAVN?PPFGNEKLTF: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize Donor13: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CASSILGWSEAFX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CSARTGDRTEAFX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tidytcells/aa/_standardize.py:80: UserWarning: Failed to standardize CASSLRTRTDTQYX: not a valid amino acid sequence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following script removes a lot of rows. They are kept and some of them get added again later\n",
      "distinct entries (all columns, keep=first). 6090 entries removed.\n",
      "removed all duplicates from distinct values (cultivated columns, keep=False). 32381 entries removed.\n",
      "paired removed entries df length: 32381\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134852/2624603129.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  duplicates_to_add = pd.concat([duplicates_to_add, group[group['is_duplicated'] == False]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32337 can be re-added to the no-duplicated dataframe\n",
      "from the plain dataset which has 54295 entries, 6134 entries have been removed.\n",
      "for paired dataset:\n",
      "size difference is: 6134\n",
      "  48161 information score cleaned: 7.147442951765952\n",
      "  54295 information score dropout: 7.243300488074408\n",
      "final_paired_df length: 48161\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for concatenation\n",
    "custom_dataset_path = f'{pipeline_data_concatenated}/{precision}/'\n",
    "\n",
    "# beta input files\n",
    "vdjdb_beta_read_path = VDJdb_cleaned_beta_output\n",
    "mcpastcr_beta_read_path = McPAS_cleaned_beta_output\n",
    "iedb_beta_read_path = IEDB_cleaned_beta_output\n",
    "# paired input files\n",
    "vdjdb_paired_read_path = VDJdb_cleaned_paired_output\n",
    "mcpastcr_paired_read_path = McPAS_cleaned_paired_output\n",
    "iedb_paired_read_path = IEDB_cleaned_paired_output\n",
    "# output files\n",
    "output_file_beta = 'beta_concatenated.tsv'\n",
    "output_file_paired = 'paired_concatenated.tsv'\n",
    "\n",
    "create_folders_if_not_exists([custom_dataset_path])\n",
    "\n",
    "%run ./data_scripts/concatDatasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_paired = f'{custom_dataset_path}/{output_file_paired}'\n",
    "concatenated_beta = f'{custom_dataset_path}/{output_file_beta}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split\n",
    "The split creates 3 datasets. Train, Validation and Test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct tcr's: 29339 from 48161\n",
      "unique tcr's: 13525 from 48161\n",
      "unique epitopes: 616 from 48161\n",
      "train data has 34636 entries\n",
      "test data has 13525 entries\n",
      "test data has 0 TPP1 tasks (unseen tcr & seen epitopes).\n",
      "test data has 11253 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 2272 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the train/test ratio is 0.7191711135566121/0.2808288864433878\n",
      "924 entries need to be shifted from train to test so the train/test ratio can be 0.7/0.3\n",
      "924 entries from train will be moved to test (TPP1)\n",
      "df_train size before: 34636\n",
      "number of tpp1 before: 0\n",
      "number of tpp2 before: 11253\n",
      "df_train size after: 33712\n",
      "number of tpp1 after: 924\n",
      "number of tpp2 after: 11253\n",
      "5164 entries will be shifted from test to train so the tpp1/tpp2 ratio can be 0.5/0.5\n",
      "5165 entries need to be shifted from train to test so the tpp1/tpp2 ratio can be 0.5/0.5\n",
      "5165 entries from train will be moved to test (TPP1)\n",
      "df_train size before: 38876\n",
      "number of tpp1 before: 924\n",
      "number of tpp2 before: 6089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_134852/4063483123.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_train = pd.concat([df_train, rows_to_move], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train size after: 33711\n",
      "number of tpp1 after: 6089\n",
      "number of tpp2 after: 6089\n",
      "train data has 33711 entries\n",
      "test data has 14450 entries\n",
      "test data has 6089 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 6089 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 2272 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the train/test ratio is 0.6999647017296152/0.3000352982703848\n",
      "test data has 7226 entries\n",
      "validation data has 7224 entries\n",
      "train data has 33711 entries\n",
      "test data has 3045 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 3891 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 290 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the test ratio is 0.849961587176346/0.150038412823654\n",
      "the validation ratio is 0.8500031145532693/0.14999688544673076\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for split of paired dataset\n",
    "input_file = concatenated_paired\n",
    "paired_output_folder = f'{pipeline_data_splitted}/{precision}/paired'\n",
    "validation_file_name = 'validation.tsv'\n",
    "test_file_name = 'test.tsv'\n",
    "train_file_name = 'train.tsv'\n",
    "aimed_test_ratio = 0.3 # this means 30% of the concatenated dataset will be for test and validation (fifty/fifty)\n",
    "\n",
    "create_folders_if_not_exists([paired_output_folder])\n",
    "\n",
    "# do the split\n",
    "%run ./data_scripts/data_preparation/split_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct tcr's: 152160 from 179822\n",
      "unique tcr's: 139540 from 179822\n",
      "unique epitopes: 678 from 179822\n",
      "train data has 40282 entries\n",
      "test data has 139540 entries\n",
      "test data has 0 TPP1 tasks (unseen tcr & seen epitopes).\n",
      "test data has 137217 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 2323 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the train/test ratio is 0.22401041029462465/0.7759895897053753\n",
      "85594 entries will be shifted from test to train so the train/test ratio can be 0.7/0.3\n",
      "25811 entries will be shifted from test to train so the tpp1/tpp2 ratio can be 0.5/0.5\n",
      "25812 entries need to be shifted from train to test so the tpp1/tpp2 ratio can be 0.5/0.5\n",
      "train data has 125875 entries\n",
      "test data has 53947 entries\n",
      "test data has 25812 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 25812 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 2323 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the train/test ratio is 0.6999977755780716/0.30000222442192837\n",
      "test data has 26974 entries\n",
      "validation data has 26973 entries\n",
      "train data has 125875 entries\n",
      "test data has 12906 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 13764 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 304 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "the test ratio is 0.8499961072616253/0.1500038927383746\n",
      "the validation ratio is 0.8500016683164463/0.14999833168355373\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for split of beta dataset\n",
    "input_file = concatenated_beta\n",
    "beta_output_folder = f'{pipeline_data_splitted}/{precision}/beta'\n",
    "aimed_test_ratio = 0.3 # this means 30% of the concatenated dataset will be for test and validation (fifty/fifty)\n",
    "\n",
    "create_folders_if_not_exists([beta_output_folder])\n",
    "\n",
    "# do the split\n",
    "%run ./data_scripts/data_preparation/split_beta.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import T5Tokenizer\n",
    "#tokenizer = T5Tokenizer.from_pretrained('Rostlab/prot_t5_xl_half_uniref50-enc', do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/BA_ZHAW/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading: Rostlab/prot_t5_xl_half_uniref50-enc\n",
      "Casting model to full precision for running on CPU ...\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for paired dataset\n",
    "read_path_train = f'{paired_output_folder}/{train_file_name}'\n",
    "read_path_test = f'{paired_output_folder}/{test_file_name}'\n",
    "read_path_validation = f'{paired_output_folder}/{validation_file_name}'\n",
    "temp_path = f'{pipeline_data_temp_bucket}/negative_samples/paired/'\n",
    "output_path = paired_output_folder  # we are not interested in the positive only data so we override them with positive/negative dataset\n",
    "train_output_name = train_file_name\n",
    "validation_output_name = validation_file_name\n",
    "test_output_name = test_file_name\n",
    "\n",
    "create_folders_if_not_exists([temp_path])\n",
    "\n",
    "%run ./data_scripts/negative_samples/negative_samples_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading: Rostlab/prot_t5_xl_half_uniref50-enc\n",
      "Casting model to full precision for running on CPU ...\n"
     ]
    }
   ],
   "source": [
    "# prepare parameters for beta dataset\n",
    "read_path_train = f'{beta_output_folder}/{train_file_name}'\n",
    "read_path_test = f'{beta_output_folder}/{test_file_name}'\n",
    "read_path_validation = f'{beta_output_folder}/{validation_file_name}'\n",
    "temp_path = f'{pipeline_data_temp_bucket}/negative_samples/beta/'\n",
    "output_path = beta_output_folder  # we are not interested in the positive only data so we override them with positive/negative dataset\n",
    "train_output_name = train_file_name\n",
    "validation_output_name = validation_file_name\n",
    "test_output_name = test_file_name\n",
    "\n",
    "create_folders_if_not_exists([temp_path])\n",
    "\n",
    "%run ./data_scripts/negative_samples/negative_samples_beta.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Classification\n",
    "The classification in the split notebook correct for positive only data. After adding negative data, some classifications might be wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paired_output_folder = f'{pipeline_data_splitted}/{precision}/paired'\n",
    "validation_file_name = 'validation.tsv'\n",
    "test_file_name = 'test.tsv'\n",
    "train_file_name = 'train.tsv'\n",
    "beta_output_folder = f'{pipeline_data_splitted}/{precision}/beta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the classification for paired data\n",
    "paired = True\n",
    "train_data_path = f'{paired_output_folder}/{train_file_name}'\n",
    "test_data_path = f'{paired_output_folder}/{test_file_name}'\n",
    "validation_data_path = f'{paired_output_folder}/{validation_file_name}'\n",
    "\n",
    "%run ./data_scripts/data_preparation/classification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gene\n",
      "./data/splitted_datasets/gene/paired/train.tsv\n",
      "       TCR_name          TRAV    TRAJ             TRA_CDR3      TRBV     TRBJ  \\\n",
      "0             1    TRAV29/DV5  TRAJ37       CAASALGNTGKLIF   TRBV4-1  TRBJ1-1   \n",
      "1             2         TRAV5  TRAJ20        CAEIRANDYKLSF   TRBV5-1  TRBJ1-2   \n",
      "2             3        TRAV17  TRAJ17     CAALDGIKAAGNKLTF    TRBV19  TRBJ1-1   \n",
      "3             4           NaN     NaN         CAFLGGANNLFF       NaN      NaN   \n",
      "4             5        TRAV17   TRAJ7       CATGLYYGNNRLAF  TRBV11-2  TRBJ2-2   \n",
      "...         ...           ...     ...                  ...       ...      ...   \n",
      "14410     14411       TRAV1-2   TRAJ3       CAVKTPSSASKIIF  TRBV20-1  TRBJ1-1   \n",
      "14411     14412         TRAV3  TRAJ13       CAVSLSGGYQKVTF    TRBV19  TRBJ1-2   \n",
      "14412     14413      TRAV38-1  TRAJ40         CAYTSGTYKYIF    TRBV27  TRBJ2-3   \n",
      "14413     14414  TRAV38-2/DV8  TRAJ39          CAYSAGNMLTF    TRBV27  TRBJ2-3   \n",
      "14414     14415        TRAV19  TRAJ57  CALSEEKVITQGGSEKLVF  TRBV10-3  TRBJ1-2   \n",
      "\n",
      "                 TRB_CDR3  TRAC  TRBC    Epitope    MHC  Binding  task  \n",
      "0          CASSPGAGDTEAFF   NaN   NaN  KLGGALQAK  HLA-A        1  TPP1  \n",
      "1             CASSLGQRYTF   NaN   NaN  KLGGALQAK  HLA-A        1  TPP1  \n",
      "2         CASKTGGALNTEAFF   NaN   NaN  KLGGALQAK  HLA-A        1  TPP1  \n",
      "3          CASSLEWGGETQYF   NaN   NaN  KLGGALQAK  HLA-A        1  TPP1  \n",
      "4       CASSLGVAGTNTGELFF   NaN   NaN  KLGGALQAK  HLA-A        1  TPP1  \n",
      "...                   ...   ...   ...        ...    ...      ...   ...  \n",
      "14410      CSAAMFTPLLEAFF   NaN   NaN  GILGFVFTL  HLA-A        0  TPP2  \n",
      "14411        CASSRENYGYTF   NaN   NaN  IVTDFSVIK  HLA-A        0  TPP2  \n",
      "14412   CASSLSDSVVGTDTQYF   NaN   NaN  CINGVCWTV  HLA-A        0  TPP2  \n",
      "14413  CASSLQGGPVASTDTQYF   NaN   NaN  GQMWLLAPR  HLA-B        0  TPP2  \n",
      "14414     CAISESSRGVDGYTF   NaN   NaN  CVNGSCFTV  HLA-A        0  TPP2  \n",
      "\n",
      "[14415 rows x 13 columns]\n",
      "train+validate data has 81381 entries\n",
      "test data has 14415 entries\n",
      "test data has 9015 TPP1 tasks (old value: 5879) (seen tcr & seen epitopes).\n",
      "test data has 4680 TPP2 tasks (old value: 7816) (unseen tcr & seen epitopes).\n",
      "test data has 412 TPP3 tasks (old value: 546) (unseen tcr & unseen epitope).\n",
      "test data has 308 TPP4 tasks (old value: 174) (seen tcr & unseen epitope).\n",
      "the train/test ratio is 0.8495239884755105/0.15047601152448953\n",
      "./data/splitted_datasets/gene/paired/test_reclassified_paired_specific.tsv\n",
      "uploading dataset to dataset-gene\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrohoari\u001b[0m (\u001b[33mpa_cancerimmunotherapy\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/PA-Cancer-Immunotherapy/BA_ZHAW/wandb/run-20241108_085044-0asjbhfy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene/runs/0asjbhfy' target=\"_blank\">lyric-monkey-8</a></strong> to <a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene' target=\"_blank\">https://wandb.ai/pa_cancerimmunotherapy/dataset-gene</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene/runs/0asjbhfy' target=\"_blank\">https://wandb.ai/pa_cancerimmunotherapy/dataset-gene/runs/0asjbhfy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./data/splitted_datasets/gene/paired)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lyric-monkey-8</strong> at: <a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene/runs/0asjbhfy' target=\"_blank\">https://wandb.ai/pa_cancerimmunotherapy/dataset-gene/runs/0asjbhfy</a><br/> View project at: <a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene' target=\"_blank\">https://wandb.ai/pa_cancerimmunotherapy/dataset-gene</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241108_085044-0asjbhfy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extended classification for paired data\n",
    "test_path = f'{paired_output_folder}/{test_file_name}'\n",
    "train_path = f'{paired_output_folder}/{train_file_name}'\n",
    "validation_path = f'{paired_output_folder}/{validation_file_name}'\n",
    "output_path = f'{paired_output_folder}/test_reclassified_paired_specific.tsv'\n",
    "paired_data_path = paired_output_folder\n",
    "alpha_cdr3_name = 'TRA_CDR3'\n",
    "beta_cdr3_name = 'TRB_CDR3'\n",
    "epitope_name = 'Epitope'\n",
    "task_name = 'task'\n",
    "\n",
    "%run ./data_scripts/data_preparation/paired_reclassification.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the classification for beta data\n",
    "paired = False\n",
    "train_data_path = f'{beta_output_folder}/{train_file_name}'\n",
    "test_data_path = f'{beta_output_folder}/{test_file_name}'\n",
    "validation_data_path = f'{beta_output_folder}/{validation_file_name}'\n",
    "\n",
    "%run ./data_scripts/data_preparation/classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two cells the classification is checked. If the output says \"Classification is correct\", everything is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train+validate data has 81381 entries\n",
      "test data has 14415 entries\n",
      "test data has 5879 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 7816 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 546 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "test data has 174 TPP4 tasks (seen tcr & unseen epitope).\n",
      "the train/test ratio is 0.8495239884755105/0.15047601152448953\n",
      "Classification is correct.\n",
      "Correctness summary:\n",
      "is_correct\n",
      "True    14415\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check task classification paired\n",
    "splitted_data_path = paired_output_folder\n",
    "\n",
    "%run ./data_scripts/data_preparation/check_task_classification_paired.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data has 251686 entries\n",
      "test data has 53858 entries\n",
      "test data has 25602 TPP1 tasks (seen tcr & seen epitopes).\n",
      "test data has 27699 TPP2 tasks (unseen tcr & seen epitopes).\n",
      "test data has 437 TPP3 tasks (unseen tcr & unseen epitope).\n",
      "test data has 120 TPP4 tasks (seen tcr & unseen epitope).\n",
      "the train/test ratio is 0.8501459364557336/0.14985406354426647\n",
      "Classification is correct.\n",
      "Correctness summary:\n",
      "is_correct\n",
      "True    53858\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check task classification beta\n",
    "splitted_data_path = beta_output_folder\n",
    "\n",
    "%run ./data_scripts/data_preparation/check_task_classification_beta.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading dataset to dataset-gene\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/PA-Cancer-Immunotherapy/BA_ZHAW/wandb/run-20241108_085111-chhm8ok1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene/runs/chhm8ok1' target=\"_blank\">deft-sunset-9</a></strong> to <a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene' target=\"_blank\">https://wandb.ai/pa_cancerimmunotherapy/dataset-gene</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene/runs/chhm8ok1' target=\"_blank\">https://wandb.ai/pa_cancerimmunotherapy/dataset-gene/runs/chhm8ok1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./data/splitted_datasets/gene/paired)... Done. 0.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deft-sunset-9</strong> at: <a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene/runs/chhm8ok1' target=\"_blank\">https://wandb.ai/pa_cancerimmunotherapy/dataset-gene/runs/chhm8ok1</a><br/> View project at: <a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene' target=\"_blank\">https://wandb.ai/pa_cancerimmunotherapy/dataset-gene</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241108_085111-chhm8ok1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# upload paired data\n",
    "path_to_data = f'{pipeline_data_splitted}/{precision}/paired'\n",
    "dataset_name = f'paired_{precision}'\n",
    "#main_project_name = os.getenv(\"MAIN_PROJECT_NAME\")\n",
    "main_project_name = f\"dataset-{precision}\"\n",
    "\n",
    "%run ./data_scripts/upload_datasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploading dataset to dataset-gene\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/PA-Cancer-Immunotherapy/BA_ZHAW/wandb/run-20241108_085117-32vfzs1r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene/runs/32vfzs1r' target=\"_blank\">good-puddle-10</a></strong> to <a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene' target=\"_blank\">https://wandb.ai/pa_cancerimmunotherapy/dataset-gene</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene/runs/32vfzs1r' target=\"_blank\">https://wandb.ai/pa_cancerimmunotherapy/dataset-gene/runs/32vfzs1r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./data/splitted_datasets/gene/beta)... Done. 0.1s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.009 MB of 0.009 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">good-puddle-10</strong> at: <a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene/runs/32vfzs1r' target=\"_blank\">https://wandb.ai/pa_cancerimmunotherapy/dataset-gene/runs/32vfzs1r</a><br/> View project at: <a href='https://wandb.ai/pa_cancerimmunotherapy/dataset-gene' target=\"_blank\">https://wandb.ai/pa_cancerimmunotherapy/dataset-gene</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241108_085117-32vfzs1r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# upload beta data\n",
    "path_to_data = f'{pipeline_data_splitted}/{precision}/beta'\n",
    "dataset_name = f'beta_{precision}'\n",
    "\n",
    "%run ./data_scripts/upload_datasets.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "11.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Sollte True zurückgeben\n",
    "print(torch.version.cuda)  # Sollte die richtige CUDA-Version anzeigen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla T4\n",
      "Loading: Rostlab/prot_t5_xl_half_uniref50-enc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:446: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "path_paired_test = f\"data/splitted_datasets/{precision}/paired/test.tsv\"\n",
    "path_paired_validation = f\"data/splitted_datasets/{precision}/paired/validation.tsv\"\n",
    "path_paired_train = f\"data/splitted_datasets/{precision}/paired/train.tsv\"\n",
    "path_beta_test = f\"data/splitted_datasets/{precision}/beta/test.tsv\"\n",
    "path_beta_validation = f\"data/splitted_datasets/{precision}/beta/validation.tsv\"\n",
    "path_beta_train = f\"data/splitted_datasets/{precision}/beta/train.tsv\"\n",
    "\n",
    "path_paired = f\"{pipeline_data}/embeddings/temp/{precision}/paired_concatenated.tsv\"\n",
    "create_folders_if_not_exists([os.path.dirname(path_paired)])\n",
    "df_paired_test = pd.read_csv(path_paired_test, sep=\"\\t\", index_col=False)\n",
    "df_paired_validation = pd.read_csv(path_paired_validation, sep=\"\\t\", index_col=False)\n",
    "df_paired_train = pd.read_csv(path_paired_train, sep=\"\\t\", index_col=False)\n",
    "df_paired = pd.concat([df_paired_test, df_paired_validation, df_paired_train])\n",
    "df_paired.to_csv(path_paired, sep=\"\\t\", index=False)\n",
    "\n",
    "# # paired\n",
    "#%run ./data_scripts/generateEmbeddings.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/TRA_paired_embeddings.npz TRA_CDR3\n",
    "#%run ./data_scripts/generateEmbeddings.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/TRB_paired_embeddings.npz TRB_CDR3\n",
    "#%run ./data_scripts/generateEmbeddings.py paired {path_paired} {pipeline_data}/embeddings/paired/{precision}/Epitope_paired_embeddings.npz Epitope\n",
    "\n",
    "path_beta = f\"{pipeline_data}/embeddings/temp/{precision}/beta_concatenated.tsv\"\n",
    "create_folders_if_not_exists([os.path.dirname(path_beta)])\n",
    "df_beta_test = pd.read_csv(path_beta_test, sep=\"\\t\", index_col=False)\n",
    "df_beta_validation = pd.read_csv(path_beta_validation, sep=\"\\t\", index_col=False)\n",
    "df_beta_train = pd.read_csv(path_beta_train, sep=\"\\t\", index_col=False)\n",
    "df_beta = pd.concat([df_beta_test, df_beta_validation, df_beta_train])\n",
    "df_beta.to_csv(path_beta, sep=\"\\t\", index=False)\n",
    "\n",
    "# beta\n",
    "#%run ./data_scripts/generateEmbeddings.py beta {path_beta} {pipeline_data}/embeddings/beta/{precision}/TRB_beta_embeddings.npz TRB_CDR3\n",
    "%run ./data_scripts/generateEmbeddings.py beta {path_beta} {pipeline_data}/embeddings/beta/{precision}/Epitope_beta_embeddings.npz Epitope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Physicochemical Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: python: command not found\n",
      "/bin/bash: line 1: python: command not found\n",
      "/bin/bash: line 1: python: command not found\n",
      "/bin/bash: line 1: python: command not found\n",
      "/bin/bash: line 1: python: command not found\n",
      "/bin/bash: line 1: python: command not found\n"
     ]
    }
   ],
   "source": [
    "!python ./data_scripts/generatePhysicoParallel.py paired {pipeline_data_splitted}/{precision}/paired test ./data/physicoProperties {precision}\n",
    "!python ./data_scripts/generatePhysicoParallel.py paired {pipeline_data_splitted}/{precision}/paired validation ./data/physicoProperties {precision}\n",
    "!python ./data_scripts/generatePhysicoParallel.py paired {pipeline_data_splitted}/{precision}/paired train ./data/physicoProperties {precision}\n",
    "\n",
    "!python ./data_scripts/generatePhysicoParallel.py beta {pipeline_data_splitted}/{precision}/beta test ./data/physicoProperties {precision}\n",
    "!python ./data_scripts/generatePhysicoParallel.py beta {pipeline_data_splitted}/{precision}/beta validation ./data/physicoProperties {precision}\n",
    "!python ./data_scripts/generatePhysicoParallel.py beta {pipeline_data_splitted}/{precision}/beta train ./data/physicoProperties {precision}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Physicochemical Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m This environment is externally managed\n",
      "\u001b[31m╰─>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n"
     ]
    }
   ],
   "source": [
    "!pip install peptides\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'peptides'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/tmp/ipykernel_48061/2556338357.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# from sklearn.preprocessing import StandardScaler\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpeptides\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'peptides'"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'peptides'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/physicoProperties\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpaired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data_scripts/scale_physicos.ipynb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data_scripts/scale_physicos.ipynb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/share/pipx/venvs/notebook/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2480\u001b[0m, in \u001b[0;36mInteractiveShell.run_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_ns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_local_scope(stack_depth)\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[0;32m-> 2480\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2484\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/share/pipx/venvs/notebook/lib/python3.12/site-packages/IPython/core/magics/execution.py:741\u001b[0m, in \u001b[0;36mExecutionMagics.run\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m preserve_keys(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    740\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshell\u001b[38;5;241m.\u001b[39muser_ns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__file__\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m filename\n\u001b[0;32m--> 741\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msafe_execfile_ipy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[38;5;66;03m# Control the response to exit() calls made by the script being run\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/pipx/venvs/notebook/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3005\u001b[0m, in \u001b[0;36mInteractiveShell.safe_execfile_ipy\u001b[0;34m(self, fname, shell_futures, raise_exceptions)\u001b[0m\n\u001b[1;32m   3003\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_cell(cell, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, shell_futures\u001b[38;5;241m=\u001b[39mshell_futures)\n\u001b[1;32m   3004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m raise_exceptions:\n\u001b[0;32m-> 3005\u001b[0m     \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39msuccess:\n\u001b[1;32m   3007\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/pipx/venvs/notebook/lib/python3.12/site-packages/IPython/core/interactiveshell.py:308\u001b[0m, in \u001b[0;36mExecutionResult.raise_error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_before_exec\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_in_exec\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/tmp/ipykernel_48061/2556338357.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# from sklearn.preprocessing import StandardScaler\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpeptides\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'peptides'"
     ]
    }
   ],
   "source": [
    "base_path = \"./data/physicoProperties\"\n",
    "chain = \"paired\"\n",
    "%run ./data_scripts/scale_physicos.ipynb\n",
    "\n",
    "chain = \"beta\"\n",
    "%run ./data_scripts/scale_physicos.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(epitope_scaler.n_features_in_)\n",
    "assert epitope_scaler.n_features_in_ == number_of_pyhsico_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Zeilen in Paired Gene: 48161\n",
      "Anzahl der Zeilen in Paired Allele: 52167\n",
      "Anzahl der Zeilen in Beta Gene: 179822\n",
      "Anzahl der Zeilen in Beta Allele: 199492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_91312/49313462.py:13: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  beta_allele_df = pd.read_csv(data_path_beta_allele, sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "# Datenlänge Überprüfung....\n",
    "\n",
    "## concatenated_datasets\n",
    "data_path_paired_gene = 'data/concatenated_datasets/gene/paired_concatenated.tsv'\n",
    "data_path_paired_allele = 'data/concatenated_datasets/allele/paired_concatenated.tsv'\n",
    "data_path_beta_gene = 'data/concatenated_datasets/gene/beta_concatenated.tsv'\n",
    "data_path_beta_allele = 'data/concatenated_datasets/allele/beta_concatenated.tsv'\n",
    "\n",
    "# Datensätze laden\n",
    "paired_gene_df = pd.read_csv(data_path_paired_gene)\n",
    "paired_allele_df = pd.read_csv(data_path_paired_allele, sep='\\t')\n",
    "beta_gene_df = pd.read_csv(data_path_beta_gene)\n",
    "beta_allele_df = pd.read_csv(data_path_beta_allele, sep='\\t')\n",
    "\n",
    "# Anzahl der Zeilen berechnen\n",
    "paired_gene_length = len(paired_gene_df)\n",
    "paired_allele_length = len(paired_allele_df)\n",
    "beta_gene_length = len(beta_gene_df)\n",
    "beta_allele_length = len(beta_allele_df)\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "print(f'Anzahl der Zeilen in Paired Gene: {paired_gene_length}')\n",
    "print(f'Anzahl der Zeilen in Paired Allele: {paired_allele_length}')\n",
    "print(f'Anzahl der Zeilen in Beta Gene: {beta_gene_length}')\n",
    "print(f'Anzahl der Zeilen in Beta Allele: {beta_allele_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Paired Gene ---\n",
      "Anzahl der Zeilen im Trainingsdatensatz: 66958\n",
      "Anzahl der Zeilen im Testdatensatz: 14415\n",
      "Anzahl der Zeilen im Validierungsdatensatz: 14423\n",
      "Gesamtanzahl der Zeilen (Train + Test + Validation): 95796\n",
      "\n",
      "--- Paired Allele ---\n",
      "Anzahl der Zeilen im Trainingsdatensatz: 72623\n",
      "Anzahl der Zeilen im Testdatensatz: 15609\n",
      "Anzahl der Zeilen im Validierungsdatensatz: 15608\n",
      "Gesamtanzahl der Zeilen (Train + Test + Validation): 103840\n",
      "\n",
      "--- Beta Gene ---\n",
      "Anzahl der Zeilen im Trainingsdatensatz: 251686\n",
      "Anzahl der Zeilen im Testdatensatz: 53858\n",
      "Anzahl der Zeilen im Validierungsdatensatz: 53859\n",
      "Gesamtanzahl der Zeilen (Train + Test + Validation): 359403\n",
      "\n",
      "--- Beta Allele ---\n",
      "Anzahl der Zeilen im Trainingsdatensatz: 279012\n",
      "Anzahl der Zeilen im Testdatensatz: 59798\n",
      "Anzahl der Zeilen im Validierungsdatensatz: 59771\n",
      "Gesamtanzahl der Zeilen (Train + Test + Validation): 398581\n",
      "\n",
      "         Dataset   Train   Test  Validation   Total\n",
      "0    Paired Gene   66958  14415       14423   95796\n",
      "1  Paired Allele   72623  15609       15608  103840\n",
      "2      Beta Gene  251686  53858       53859  359403\n",
      "3    Beta Allele  279012  59798       59771  398581\n"
     ]
    }
   ],
   "source": [
    "# Datenlänge Überprüfung....\n",
    "\n",
    "## splitted datasets\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Beispielpfade für Train-, Test-, und Validierungsdatensätze für alle vier Kategorien\n",
    "base_path = 'data/splitted_datasets'\n",
    "\n",
    "# Definierte Pfade für alle vier Kategorien\n",
    "datasets = {\n",
    "    \"paired_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/paired/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/paired/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/paired/validation.tsv\"\n",
    "    },\n",
    "    \"paired_allele\": {\n",
    "        \"train\": f\"{base_path}/allele/paired/train.tsv\",\n",
    "        \"test\": f\"{base_path}/allele/paired/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/allele/paired/validation.tsv\"\n",
    "    },\n",
    "    \"beta_gene\": {\n",
    "        \"train\": f\"{base_path}/gene/beta/train.tsv\",\n",
    "        \"test\": f\"{base_path}/gene/beta/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/gene/beta/validation.tsv\"\n",
    "    },\n",
    "    \"beta_allele\": {\n",
    "        \"train\": f\"{base_path}/allele/beta/train.tsv\",\n",
    "        \"test\": f\"{base_path}/allele/beta/test.tsv\",\n",
    "        \"validation\": f\"{base_path}/allele/beta/validation.tsv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Berechnung der Anzahl der Zeilen für jedes Set\n",
    "results = {}\n",
    "for dataset_name, paths in datasets.items():\n",
    "    # Daten laden\n",
    "    train_df = pd.read_csv(paths[\"train\"], sep='\\t')\n",
    "    test_df = pd.read_csv(paths[\"test\"], sep='\\t')\n",
    "    validation_df = pd.read_csv(paths[\"validation\"], sep='\\t')\n",
    "    \n",
    "    # Anzahl der Zeilen berechnen\n",
    "    train_length = len(train_df)\n",
    "    test_length = len(test_df)\n",
    "    validation_length = len(validation_df)\n",
    "    total_length = train_length + test_length + validation_length\n",
    "\n",
    "    # Ergebnisse speichern\n",
    "    results[dataset_name] = {\n",
    "        \"Train\": train_length,\n",
    "        \"Test\": test_length,\n",
    "        \"Validation\": validation_length,\n",
    "        \"Total\": total_length\n",
    "    }\n",
    "\n",
    "# Ergebnisse anzeigen\n",
    "for dataset, lengths in results.items():\n",
    "    print(f'--- {dataset.replace(\"_\", \" \").title()} ---')\n",
    "    print(f'Anzahl der Zeilen im Trainingsdatensatz: {lengths[\"Train\"]}')\n",
    "    print(f'Anzahl der Zeilen im Testdatensatz: {lengths[\"Test\"]}')\n",
    "    print(f'Anzahl der Zeilen im Validierungsdatensatz: {lengths[\"Validation\"]}')\n",
    "    print(f'Gesamtanzahl der Zeilen (Train + Test + Validation): {lengths[\"Total\"]}\\n')\n",
    "\n",
    "# Optional: Ergebnisse in einer Übersichtstabelle darstellen\n",
    "summary_data = []\n",
    "for dataset, lengths in results.items():\n",
    "    summary_data.append({\n",
    "        \"Dataset\": dataset.replace(\"_\", \" \").title(),\n",
    "        \"Train\": lengths[\"Train\"],\n",
    "        \"Test\": lengths[\"Test\"],\n",
    "        \"Validation\": lengths[\"Validation\"],\n",
    "        \"Total\": lengths[\"Total\"]\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(summary_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
